{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9d96f6-e310-4c84-88bd-6d716e290b2a",
   "metadata": {},
   "source": [
    "# Multi-GPU\n",
    "\n",
    "In this notebook, we learn how to train a model with multiple devices in TensorFlow.\n",
    "\n",
    "In machine learning, there are two types of parallelism:\n",
    "\n",
    "**Data parallelism**: A single model is replicated on multiple devices, each processing different batches of data, and the gradients obtained from backprop are merged before updating the model weights. \n",
    "\n",
    "**Model parallelism**: Different parts of a single model run on different devices. This is used for very large models (such as LLMs).\n",
    "\n",
    "We will consider both in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f6a07a-892f-48ce-b382-7aa1c64687c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 23:18:15.140478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-05 23:18:15.305350: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-05 23:18:15.345636: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-05 23:18:16.109135: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-09-05 23:18:16.109281: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-09-05 23:18:16.109295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd36181-a948-43e0-8975-4b6133a454a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create virtual GPUs\n",
    "\n",
    "In this training course, we may not able to provide multiple physical GPUs to everyone. Therefore, we use virtual or logical GPUs instead. TensorFlow will see these virtual GPUs as logically independent, so the code in the later sections should also work on multiple physical GPUs.\n",
    "\n",
    "Now we create 4 virtual GPUs, each taking 1GB memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3aebf48-1151-46ae-a9ff-6cbef47034b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-05 23:18:17.050133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.083707: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.083981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.085013: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-05 23:18:17.085887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.086129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.086279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.666408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.666699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.666855: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.667023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.667175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.667321: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.667447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1024 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "2023-09-05 23:18:17.667920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.668086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 1024 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "2023-09-05 23:18:17.668212: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.668337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 1024 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "2023-09-05 23:18:17.668576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-05 23:18:17.668710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 1024 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# specify number of GPUs\n",
    "N_GPUS = 4\n",
    "MEM_GPU = 1024\n",
    "\n",
    "physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_logical_device_configuration(\n",
    "        physical_gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=MEM_GPU)] * N_GPUS)\n",
    "\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04278e4d-b150-46c9-8b3a-cc795b247612",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical GPUs:\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "\n",
      "Logical GPUs:\n",
      "LogicalDevice(name='/device:GPU:0', device_type='GPU')\n",
      "LogicalDevice(name='/device:GPU:1', device_type='GPU')\n",
      "LogicalDevice(name='/device:GPU:2', device_type='GPU')\n",
      "LogicalDevice(name='/device:GPU:3', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# print the GPUs\n",
    "print('Physical GPUs:')\n",
    "for device in physical_gpus:\n",
    "    print(device)\n",
    "\n",
    "print('\\nLogical GPUs:')\n",
    "for device in logical_gpus:\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026a6975-5204-459b-8983-1a20977ec5d0",
   "metadata": {},
   "source": [
    "## Data parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b6243-dc19-4b37-9877-7a9e1bf44b96",
   "metadata": {},
   "source": [
    "First, we define the functions to create the model and datasets. If you are unsure about any lines, please revisit `DNN/DNN_basics.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec332dc-7df4-429e-8a92-a22dc4b841d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    # Make a simple 4-layer densely-connected neural network.\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(10)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa3f06e2-559e-474c-9069-beccc9cfde6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    batch_size = 256\n",
    "    num_val_samples = 10000\n",
    "\n",
    "    # Return the MNIST dataset in the form of a `tf.data.Dataset`.\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Preprocess the data (these are Numpy arrays)\n",
    "    x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255\n",
    "    y_train = y_train.astype(\"float32\")\n",
    "    y_test = y_test.astype(\"float32\")\n",
    "\n",
    "    # Reserve num_val_samples samples for validation\n",
    "    x_val = x_train[-num_val_samples:]\n",
    "    y_val = y_train[-num_val_samples:]\n",
    "    x_train = x_train[:-num_val_samples]\n",
    "    y_train = y_train[:-num_val_samples]\n",
    "    \n",
    "    # datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batch_size)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "    \n",
    "    # disable auto-share policy for a tensorflow issue. This may be fixed in the future.\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "    train_dataset = train_dataset.with_options(options)\n",
    "    val_dataset = val_dataset.with_options(options)\n",
    "    test_dataset = test_dataset.with_options(options)\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98009533-4b02-465d-beec-6f0ecfa1d159",
   "metadata": {},
   "source": [
    "Next, from `tf.distribute`, we create a strategy for parallelism. Here we use `MirroredStrategy`, which is for synchronous training across multiple replicas on one machine. For distributed training on multiple machines, one needs `MultiWorkerMirroredStrategy` and select devices from the machines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cafb9337-b72b-44cc-be6d-c8235611d9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Number of devices: 4\n"
     ]
    }
   ],
   "source": [
    "# devices=None will use all avialable GPUs; \n",
    "# devices=['GPU:0', 'GPU:1'] will use two GPUs\n",
    "strategy = tf.distribute.MirroredStrategy(devices=None)\n",
    "print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f6ae4-620e-4981-a9c9-124dfa942f77",
   "metadata": {},
   "source": [
    "Now we create the model within the scope of the `MirroredStrategy`. Note that **this is the only difference from the single GPU case**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2e1b13-2f8b-4229-a562-16e74bb22161",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = get_compiled_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fe010f-ef34-4880-9c1b-f55d4953c4c8",
   "metadata": {},
   "source": [
    "Now start training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e47ac10-e199-4718-8cd5-1770bb953c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "196/196 [==============================] - 8s 19ms/step - loss: 0.3768 - sparse_categorical_accuracy: 0.8951 - val_loss: 0.1633 - val_sparse_categorical_accuracy: 0.9551\n",
      "Epoch 2/2\n",
      "196/196 [==============================] - 2s 11ms/step - loss: 0.1499 - sparse_categorical_accuracy: 0.9556 - val_loss: 0.1149 - val_sparse_categorical_accuracy: 0.9665\n",
      "40/40 [==============================] - 1s 7ms/step - loss: 0.1184 - sparse_categorical_accuracy: 0.9645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11843422800302505, 0.9645000100135803]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on all available devices.\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597df6b2-2540-46e6-9d45-91b09b0874cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model parallelism\n",
    "\n",
    "In this section, we demonstrate model parallelism by manually assigning each layer a different device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d63f66b7-fae0-4463-a32a-5b6c5491ac7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input and first hidden on GPU:0\n",
    "with tf.device(\"GPU:0\"):\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
    "# second hidden on GPU:1\n",
    "with tf.device(\"GPU:1\"):\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "# third hidden on GPU:2\n",
    "with tf.device(\"GPU:2\"):\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "# output on GPU:3\n",
    "with tf.device(\"GPU:3\"):\n",
    "    outputs = keras.layers.Dense(10)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df00c87-ea68-4202-b745-9d61eebef7fc",
   "metadata": {},
   "source": [
    "Now start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec14efa9-c138-46ec-9dec-2218c773480a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 0.3652 - sparse_categorical_accuracy: 0.8956 - val_loss: 0.1417 - val_sparse_categorical_accuracy: 0.9598\n",
      "Epoch 2/2\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 0.1282 - sparse_categorical_accuracy: 0.9612 - val_loss: 0.1008 - val_sparse_categorical_accuracy: 0.9703\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.1029 - sparse_categorical_accuracy: 0.9671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10288294404745102, 0.9671000242233276]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on all available devices.\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26318644-eff6-4250-afac-6a99530a828d",
   "metadata": {},
   "source": [
    "Final notes:\n",
    "1. On multiple machines, the GPUs can be selected with full paths with machine names. \n",
    "2. Overheads for data transfer between physical GPUs can be greatly reduced by new communication hardware technologies such as NVLink and NVSwitch.\n",
    "3. Model distribution is more common in LLMs. Luckily, `HuggingFace` provides automatic device mapping based on layer sizes and available devices, so one usually does not need to configure model distribution manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ffef30-191e-4a0b-9227-3a7618175ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Variational Autoencoders: Advanced topics\n", "\n", "In this notebook, we will learn two useful extensions of VAEs: the disentangled VAEs ($\\beta$-VAEs) and the conditional VAEs. You should have completed [VAE_basics.ipynb](VAE_basics.ipynb) before attempting this notebook."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras import layers\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use('ggplot')\n", "\n", "\n", "# need certainty to explain some of the results\n", "import random as python_random\n", "python_random.seed(0)\n", "np.random.seed(0)\n", "tf.random.set_seed(0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# The Dataset\n", "\n", "In this notebook, we will use the `mnist-digits` dataset. It is simpler than the `mnist-fashion` dataset, allowing us to use only two latent features so that we can conveniently visualise and examine the encodings distribution in the latent space."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# load dataset\n", "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n", "\n", "# normalise images\n", "train_images = train_images / 255.0\n", "test_images = test_images / 255.0\n", "\n", "# print info\n", "print(\"Number of training data: %d\" % len(train_labels))\n", "print(\"Number of test data: %d\" % len(test_labels))\n", "print(\"Image pixels: %s\" % str(train_images[0].shape))\n", "print(\"Number of classes: %d\" % (np.max(train_labels) + 1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Disentangled Variational Autoencoders\n", "\n", "Disentangled VAEs or $\\beta$-VAEs are an extended type of VAEs. As explained before, the loss of a VAE consists of two parts, the reconstruction error and the KL divergence: the former tries to fit the network with data and the later to regularise the latent space. It is thus natural to introduce a hyperparameter to weigh these two forces, which gives rise to a disentangled VAE. As you can see in the following cartoon, a factor $\\beta$ is applied to the KL divergence, and this $\\beta$ is usually greater than 1. In practice, the value of $\\beta$ has to be tuned for the best quality of content generation.\n", "\n", "So, why we call it \"disentangled\"? Consider the limit $\\beta\\rightarrow\\infty$: in this case, the distributions of all the latent features will simply converge to $\\mathcal{N}(0,1)$, regardless of the data, so they are completely disentangled. In other words, the latent features become increasingly disentangled as $\\beta$ increases. \n", "\n", "\n", "![bvae.png](https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/bvae.png?raw=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Implement a $\\beta$-VAE\n", "\n", "It is straightforward to generalise our VAE implementation: just adding $\\beta$ as a hyperparameter to the constructor of the `VAE` class and apply it to the KL divergence in the `train_step()` method. The differences are highlighted in the code.\n", "\n", "\n", "### The encoder and decoder\n", "\n", "They are exactly the same as before in the VAE example."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["latent_dim = 2\n", "# sampling z with (z_mean, z_log_var)\n", "class Sampling(layers.Layer):\n", "    def call(self, inputs):\n", "        z_mean, z_log_var = inputs\n", "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n", "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n", "    \n", "# build the encoder\n", "image_input = keras.Input(shape=(28, 28))\n", "x = layers.Flatten()(image_input)\n", "x = layers.Dense(128, activation='relu')(x)\n", "x = layers.Dense(16, activation=\"relu\")(x)\n", "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n", "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n", "z_output = Sampling()([z_mean, z_log_var])\n", "encoder_BVAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n", "\n", "# build the decoder\n", "z_input = keras.Input(shape=(latent_dim,))\n", "x = layers.Dense(16, activation=\"relu\")(z_input)\n", "x = layers.Dense(128, activation=\"relu\")(x)\n", "x = layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n", "image_output = layers.Reshape((28, 28))(x)\n", "decoder_BVAE = keras.Model(z_input, image_output)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The $\\beta$-VAE\n", "\n", "We will use $\\beta=10$ for demonstration. Try some other values (particularly a very large value such as 1000) and see what happens."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# BVAE class\n", "class BVAE(keras.Model):\n", "    # constructor\n", "    ########################################################\n", "    ######## NEW: passing beta as an extra argument ########\n", "    ########################################################\n", "    def __init__(self, encoder, decoder, beta, **kwargs):\n", "        super(BVAE, self).__init__(**kwargs)\n", "        self.encoder = encoder\n", "        self.decoder = decoder\n", "        self.beta = beta\n", "\n", "    # customise train_step() to implement the loss \n", "    def train_step(self, x):\n", "        if isinstance(x, tuple):\n", "            x = x[0]\n", "        with tf.GradientTape() as tape:\n", "            # encoding\n", "            z_mean, z_log_var, z = self.encoder(x)\n", "            # decoding\n", "            x_prime = self.decoder(z)\n", "            # reconstruction error by binary crossentropy loss\n", "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, x_prime)) * 28 * 28\n", "            # KL divergence\n", "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n", "            # loss = reconstruction error + KL divergence\n", "            #######################################\n", "            ######## NEW: scale KL by beta ########\n", "            #######################################\n", "            loss = reconstruction_loss + self.beta * kl_loss\n", "        # apply gradient\n", "        grads = tape.gradient(loss, self.trainable_weights)\n", "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n", "        # return loss for metrics log\n", "        return {\"loss\": loss,\n", "                \"reconstruction_loss\": reconstruction_loss,\n", "                \"kl_loss\": kl_loss}\n", "\n", "# build the BVAE\n", "########################################\n", "######## NEW: pass beta to BVAE ########\n", "########################################\n", "bvae_model = BVAE(encoder_BVAE, decoder_BVAE, beta=10.)\n", "\n", "# compile the BVAE\n", "bvae_model.compile(optimizer=keras.optimizers.Adam())\n", "\n", "# train the BVAE\n", "bvae_model.fit(train_images, train_images, epochs=50, batch_size=128)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Evaluate the results\n", "\n", "The same as before, we can evaluate the latent distributions and generate new images using our $\\beta$-VAE. As expected, the distributions become more close to normal distributions because $\\beta=10$. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# scatter plot of encodings in the latent space\n", "def scatter_plot_encodings_latent(encodings, labels):\n", "    plt.figure(dpi=100)\n", "    scat = plt.scatter(encodings[:, 0], encodings[:, 1], c=labels, s=.5, cmap='Paired')\n", "    plt.gca().add_artist(plt.legend(*scat.legend_elements(), \n", "                         title='Image labels', bbox_to_anchor=(1.5, 1.)))\n", "    plt.xlabel('Feature X')\n", "    plt.ylabel('Feature Y')\n", "    plt.gca().set_aspect(1)\n", "    plt.show()\n", "    \n", "# histogram plot of encodings in the latent space\n", "def hist_plot_encodings_latent(encodings, labels, digit, dim, ax):\n", "    # extract\n", "    encodings_digit = encodings[labels == digit, dim]\n", "    # histogram\n", "    ax.hist(encodings_digit, bins=60, density=True, color=['g', 'b'][dim], alpha=.5)\n", "    # mean and std dev\n", "    mean = np.mean(encodings_digit)\n", "    std = np.std(encodings_digit)\n", "    ax.axvline(mean, c='r')\n", "    ax.set_xlabel('Digit %d, Feature %s\\n~${\\cal N}(\\mu=%.1f, \\sigma=%.1f)$' % \n", "                  (digit, ['X', 'Y'][dim], mean, std), c='k')\n", "    \n", "# generate images from the latent space\n", "def generate_images_latent(decoder, x0, x1, dx, y0, y1, dy):\n", "    # uniformly sample the latent space\n", "    nx = round((x1 - x0) / dx) + 1\n", "    ny = round((y1 - y0) / dy) + 1\n", "    grid_x = np.linspace(x0, x1, nx)\n", "    grid_y = np.linspace(y1, y0, ny)\n", "    latent = np.array(np.meshgrid(grid_x, grid_y)).reshape(2, nx * ny).T\n", "\n", "    # decode images\n", "    decodings = decoder.predict(latent)\n", "    \n", "    # display a (nx, ny) 2D manifold of digits\n", "    figure = np.zeros((28 * ny, 28 * nx))\n", "    for iy in np.arange(ny):\n", "        for ix in np.arange(nx):\n", "            figure[iy * 28 : (iy + 1) * 28, ix * 28 : (ix + 1) * 28] = decodings[iy * nx + ix]\n", "            \n", "    # plot figure\n", "    plt.figure(dpi=100, figsize=(nx / 3, ny / 3))\n", "    plt.xticks(np.arange(28 // 2, nx * 28 + 28 // 2, 28), np.round(grid_x, 1), rotation=90)\n", "    plt.yticks(np.arange(28 // 2, ny * 28 + 28 // 2, 28), np.round(grid_y, 1))\n", "    plt.xlabel('Feature X')\n", "    plt.ylabel('Feature Y')\n", "    plt.imshow(figure, cmap=\"Greys_r\")\n", "    plt.grid(False)\n", "    plt.show()\n", "    \n", "# encode images by BVAE\n", "train_encodings_BVAE = encoder_BVAE.predict(train_images)\n", "\n", "# scatter plot of encodings by BVAE\n", "scatter_plot_encodings_latent(train_encodings_BVAE[2], train_labels)\n", "\n", "# histogram plot of encodings by BVAE\n", "fig, axes = plt.subplots(5, 4, dpi=100, figsize=(15, 12), sharex=True)\n", "plt.subplots_adjust(hspace=.4)\n", "for digit in range(10):\n", "    hist_plot_encodings_latent(train_encodings_BVAE[2], train_labels, digit, 0, \n", "                               axes[digit // 2, digit % 2 * 2 + 0])\n", "    hist_plot_encodings_latent(train_encodings_BVAE[2], train_labels, digit, 1, \n", "                               axes[digit // 2, digit % 2 * 2 + 1])\n", "plt.show()\n", "\n", "# generate images by BVAE\n", "generate_images_latent(decoder_BVAE, x0=-2, x1=2, dx=.1, y0=-2, y1=2, dy=.1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Conditional Variational Autoencoders\n", "\n", "In many applications, we hope to generate contents based on the labels. For example, we hope to generate images for a given digit from `mnist-digits`. Conditional VAEs serve this purpose. In a conditional VAE, the labels are sent to both the encoder and the decoder as an extra input, as shown in the following figure. \n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop-v3/blob/master/course_3.0_with_solutions/markdown_pic/cvaee.png?raw=1\" width=100% height=100% />"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Implement a conditional VAE\n", "\n", "The simplest way to implement a conditional VAE is to concatenate the labels to both the input data $x$ and the latent representation $z$. We will use this approach for our implementation. The one-hot encodings of the labels will be concatenated, which is a vector of size 10 (the condition dimension). \n", "\n", "### The encoder\n", "\n", "The encoder is the same as that of the VAE except that\n", "\n", "1. the input size is increased by the condition dimension; \n", "2. we no longer flatten the images inside the network because of the concatenation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# dimension of condition\n", "condition_dim = 10\n", "\n", "# encoder\n", "###############################################################\n", "######## NEW: input size is increased by condition_dim ########\n", "###############################################################\n", "image_input = keras.Input(shape=(28 * 28 + condition_dim))\n", "x = layers.Dense(128, activation='relu')(image_input)\n", "x = layers.Dense(16, activation=\"relu\")(x)\n", "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n", "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n", "z_output = Sampling()([z_mean, z_log_var])\n", "encoder_CVAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n", "encoder_CVAE.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The decoder\n", "\n", "The same as the encoder, the input size of the decoder is increased by the condition dimension, and the reconstructed images will be flattened externally."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# decoder\n", "###############################################################\n", "######## NEW: input size is increased by condition_dim ########\n", "###############################################################\n", "z_input = keras.Input(shape=(latent_dim + condition_dim,))\n", "x = layers.Dense(16, activation=\"relu\")(z_input)\n", "x = layers.Dense(128, activation=\"relu\")(x)\n", "image_output = layers.Dense(28 * 28, activation=\"sigmoid\")(x)\n", "decoder_CVAE = keras.Model(z_input, image_output)\n", "decoder_CVAE.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### The conditional VAE\n", "\n", "The conditional VAE is the same as the VAE except that\n", "\n", "1. the condition dimension is passed to the constructor;\n", "2. the labels are concatenated to the latent features before they are decoded;\n", "3. the labels are truncated from the input data to compute the reconstruction error."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# CVAE class\n", "class CVAE(keras.Model):\n", "    # constructor\n", "    #################################################################\n", "    ######## NEW: passing condition_dim as an extra argument ########\n", "    #################################################################\n", "    def __init__(self, encoder, decoder, condition_dim, **kwargs):\n", "        super(CVAE, self).__init__(**kwargs)\n", "        self.encoder = encoder\n", "        self.decoder = decoder\n", "        self.condition_dim = condition_dim\n", "\n", "    # customise train_step() to implement the loss \n", "    def train_step(self, x):\n", "        if isinstance(x, tuple):\n", "            x = x[0]\n", "        with tf.GradientTape() as tape:\n", "            # encoding\n", "            z_mean, z_log_var, z = self.encoder(x)\n", "            ####################################################################\n", "            ######## NEW: apply conditions to encodings before decoding ########\n", "            ####################################################################\n", "            z_cond = tf.concat([z, x[:, -self.condition_dim:]], axis=1)\n", "            # decoding\n", "            x_prime = self.decoder(z_cond)\n", "            ###################################################################\n", "            ######## NEW: truncate conditions for reconstruction error ########\n", "            ###################################################################\n", "            # reconstruction error by binary crossentropy loss\n", "            reconstruction_loss = tf.reduce_mean(\n", "                keras.losses.binary_crossentropy(x[:, :-self.condition_dim], x_prime)) * 28 * 28\n", "            # KL divergence\n", "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n", "            # loss = reconstruction error + KL divergence\n", "            loss = reconstruction_loss + kl_loss\n", "        # apply gradient\n", "        grads = tape.gradient(loss, self.trainable_weights)\n", "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n", "        # return loss for metrics log\n", "        return {\"loss\": loss,\n", "                \"reconstruction_loss\": reconstruction_loss,\n", "                \"kl_loss\": kl_loss}\n", "\n", "# build the CVAE\n", "cvae_model = CVAE(encoder_CVAE, decoder_CVAE, condition_dim)\n", "\n", "# compile the CVAE\n", "cvae_model.compile(optimizer=keras.optimizers.Adam())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Concatenate the labels\n", "\n", "Before training, we need to concatenate the images and their labels to generate the training data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# one-hot encoding labels\n", "train_labels_onehot = np.eye(10)[train_labels]\n", "\n", "# flatten images\n", "train_images_flattened = train_images.reshape((len(train_images), 28 * 28))\n", "\n", "# concatenate labels to images\n", "train_images_conditioned = np.concatenate((train_images_flattened, train_labels_onehot), axis=1)\n", "\n", "# train the VAE\n", "cvae_model.fit(train_images_conditioned, train_images_flattened, \n", "               epochs=50, batch_size=128)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Evaluate the results\n", "\n", "The latent distributions can be assessed in the same way. Note that *each digit now has its own latent distributions because of the conditioning* (all resemble a $\\mathcal{N}(0,1)$), so the latent space is highly regular and disentangled. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# encode images by CVAE\n", "train_encodings_CVAE = encoder_CVAE.predict(train_images_conditioned)\n", "\n", "# scatter plot of encodings by CVAE\n", "scatter_plot_encodings_latent(train_encodings_CVAE[2], train_labels)\n", "\n", "# histogram plot of xencodings by CVAE\n", "fig, axes = plt.subplots(5, 4, dpi=100, figsize=(15, 15), sharex=True)\n", "plt.subplots_adjust(hspace=.4)\n", "for digit in range(10):\n", "    hist_plot_encodings_latent(train_encodings_CVAE[2], train_labels, digit, 0, \n", "                               axes[digit // 2, digit % 2 * 2 + 0])\n", "    hist_plot_encodings_latent(train_encodings_CVAE[2], train_labels, digit, 1, \n", "                               axes[digit // 2, digit % 2 * 2 + 1])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we can generate new images for each digit:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# generate images from the conditioned latent space\n", "def generate_images_conditioned_latent(decoder, x0, x1, dx, y0, y1, dy, condition_digit, ax):\n", "    # uniformly sample the latent space\n", "    nx = round((x1 - x0) / dx) + 1\n", "    ny = round((y1 - y0) / dy) + 1\n", "    grid_x = np.linspace(x0, x1, nx)\n", "    grid_y = np.linspace(y1, y0, ny)\n", "    latent = np.array(np.meshgrid(grid_x, grid_y)).reshape(2, nx * ny).T\n", "    \n", "    # condition the latent space\n", "    condiont_one_hot = np.eye(10)[condition_digit]\n", "    latent = np.concatenate((latent, np.repeat([condiont_one_hot], len(latent), axis=0)), axis=1)\n", "\n", "    # decode images\n", "    decodings = decoder.predict(latent).reshape((len(latent), 28, 28))\n", "    \n", "    # display a (nx, ny) 2D manifold of digits\n", "    figure = np.zeros((28 * ny, 28 * nx))\n", "    for iy in np.arange(ny):\n", "        for ix in np.arange(nx):\n", "            figure[iy * 28 : (iy + 1) * 28, ix * 28 : (ix + 1) * 28] = decodings[iy * nx + ix]\n", "            \n", "    # plot figure\n", "    ax.axis('off')\n", "    ax.imshow(figure, cmap=\"Greys_r\")\n", "    \n", "# generate images by CVAE\n", "fig, axes = plt.subplots(5, 2, dpi=100, figsize=(10, 30), )\n", "for digit in range(10):\n", "    generate_images_conditioned_latent(decoder_CVAE, x0=-2, x1=2, dx=.2, y0=-2, y1=2, dy=.2,\n", "                                        condition_digit=digit, ax=axes[digit // 2, digit % 2])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises\n", "\n", "Use a VAE, $\\beta$-VAE or conditional VAE to generate images from the `mnist-fashion` dataset. In this case, a latent dimension of two will be insufficient. For a larger latent dimension, you can use the above code for building and training the models without any changes. However, you have to generalise the code for plotting data distributions and generalising new images."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}
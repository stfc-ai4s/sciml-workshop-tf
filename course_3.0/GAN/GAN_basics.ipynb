{"cells": [{"cell_type": "markdown", "metadata": {"id": "IL9SW6kO1Pea"}, "source": ["# Generative Adversarial Networks: the Basics\n", "\n", "Generative Adversarial Networks (GANs) represents one of the most exciting recent innovation in deep learning. GANs were originally introduced by Ian Goodfellow and Yoshua Bengio from the University of Montreal, in 2014 and Yann LeCun considered them as \u2018the most interesting idea in the last 10 years in ML\u2019 .\n", "\n", "A GAN is a generative model in which two neural networks are competing in a typical game theory scenario. The first neural network is the generator, responsible of generating new synthetic data instances that resemble your training data, while its adversary, the discriminator tries to distinguish between real (training) and fake (artificially generated) samples generated by the generator. The mission of the generator is to try fooling the discriminator, and the discriminator tries to resist from being fooled. That\u2019s why the system as a whole is described as adversarial. ([Source](https://towardsdatascience.com/generative-adversarial-networks-gans-2231c5943b11))\n", "\n", "\n", "<img src=\"https://miro.medium.com/max/2000/1*XKanAdkjQbg1eDDMF2-4ow.png\" alt=\"gan.png\" style=\"width: 700px;\"/>\n", "\n", "<img src=\"https://miro.medium.com/max/1400/1*FbQLpEVQKsMSK-c7_5KcWw.png\" alt=\"gan.png\" style=\"width: 700px;\"/>\n", "\n", "([Source](https://towardsdatascience.com/generative-adversarial-networks-explained-34472718707a))\n", "\n", "\n", "\n", "\n", "* Further reading:\n", "\n", "    \ud83d\udca1 [ThisXDoesNotExist.com](https://thisxdoesnotexist.com/) displays realistic and diverse contents generated by GANs.\n", "\n", "    \ud83d\udca1 [NIPS 2016 Tutorial:Generative Adversarial Networks](https://arxiv.org/pdf/1701.00160.pdf).\n", "\n", "\n", "In this notebook, we will train a GAN to generate handwritten digits based on the `mnist-digits` dataset. This notebook is adapted from the [TensorFlow tutorial on GANs](https://www.tensorflow.org/tutorials/generative/dcgan). "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "fhRLCYd81Peb", "outputId": "d61a849d-9314-4890-94f3-b51a53446179"}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "import tensorflow.keras.layers as layers\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import time\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {"id": "iagz8L7p1Pec"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "TJweXsVG1Pec"}, "source": ["# The Dataset\n", "\n", "We start by loading the `mnist` dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "FN5OkfKt1Pec", "outputId": "d9f9a88a-67fd-45f8-a89b-d081760c5c90"}, "outputs": [], "source": ["# load dataset\n", "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()\n", "\n", "# normalise images to [-1, 1]\n", "train_images = train_images / 255.0 * 2 - 1.\n", "test_images = test_images / 255.0 * 2 - 1.\n", "\n", "# print info\n", "print(\"Number of training data: %d\" % len(train_labels))\n", "print(\"Number of test data: %d\" % len(test_labels))\n", "print(\"Image pixels: %s\" % str(train_images[0].shape))\n", "print(\"Number of classes: %d\" % (np.max(train_labels) + 1))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 712}, "id": "_yuS7hPJ1Ped", "outputId": "8f8866a1-e2d5-4f67-a6a4-dfa856539969"}, "outputs": [], "source": ["# function to plot an image in a subplot\n", "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n", "    plt.subplot(nrows, ncols, iplot + 1)\n", "    plt.imshow(image, cmap=plt.cm.binary)\n", "    plt.xlabel(label, c='k', fontsize=12)\n", "    plt.title(label2, c=label2_color, fontsize=12, y=-0.33)\n", "    plt.xticks([])\n", "    plt.yticks([])\n", "    \n", "# ramdomly plot some images and their labels\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n", "    subplot_image(train_images[idata], train_labels[idata], nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "gXtKCXRf1Ped"}, "source": ["---\n", "\n", "# Create a GAN\n", "\n", "### The generator\n", "\n", "First, we create the generator network. The generator generates an image by upsampling a **seed** (a random noise vector) several times. Each upsampling is conducted by a `Conv2DTranspose` layer. We start with a `Dense` layer that upscales the seed into a proper size such that it can be reshaped into a stack of small images; next, several `Conv2DTranspose` layers are followed to upsample the small images into the original image size. All layers use `LeakyReLU` for activation except the output layer, which uses `tanh` (note that we have normalised the pixel values to [-1, 1] in the previous section).\n", "\n", "**Takeaway**: the generator transforms a seed (a random noise vector) into an image."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "87jYoUd-1Ped"}, "outputs": [], "source": ["def make_generator_model(noise_size=100, image_size=(28,28), n_filters=(256,128,64)):\n", "    '''\n", "    Create a generator, hardcoded with three Conv2DTranspose layers.\n", "    \n", "    :param noise_size: size of the seed vector\n", "    :param image_size: size of the image (both width and height must divide 4)\n", "    :param n_filters: number of filters in each layer\n", "    :return: the generator model\n", "    '''\n", "    \n", "    # sequential model\n", "    model = tf.keras.Sequential()\n", "    \n", "    # Dense\n", "    # input shape: (100,)\n", "    # output shape: (7*7*256,)\n", "    w = image_size[0] // 4\n", "    h = image_size[1] // 4\n", "    model.add(layers.Dense(w*h*n_filters[0], use_bias=False, \n", "                           input_shape=(noise_size,)))\n", "    model.add(layers.BatchNormalization())\n", "    model.add(layers.LeakyReLU())\n", "\n", "    # Reshape\n", "    # input shape: (7*7*256,)\n", "    # output shape: (7, 7, 256)\n", "    model.add(layers.Reshape((w, h, n_filters[0])))\n", "\n", "    # Conv2DTranspose\n", "    # input shape: (7, 7, 256)\n", "    # output shape: (7, 7, 128)\n", "    model.add(layers.Conv2DTranspose(n_filters[1], (5, 5), strides=(1, 1), \n", "                                     padding='same', use_bias=False))\n", "    model.add(layers.BatchNormalization())\n", "    model.add(layers.LeakyReLU())\n", "\n", "    # Conv2DTranspose\n", "    # input shape: (7, 7, 128)\n", "    # output shape: (14, 14, 64)\n", "    model.add(layers.Conv2DTranspose(n_filters[2], (5, 5), strides=(2, 2), \n", "                                     padding='same', use_bias=False))\n", "    model.add(layers.BatchNormalization())\n", "    model.add(layers.LeakyReLU())\n", "\n", "    # Conv2DTranspose\n", "    # input shape: (14, 14, 64)\n", "    # output shape: (28, 28, 1)\n", "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), \n", "                                     padding='same', use_bias=False, activation='tanh'))\n", "    \n", "    # input: (100,)\n", "    # output: (28, 28, 1)\n", "    return model"]}, {"cell_type": "markdown", "metadata": {"id": "wAuPKAHE1Pee"}, "source": ["Here we can use the **untrained** generator to generate an image. This is helpful to validate the layer sizes in the generator."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 338}, "id": "3yhkJjQj1Pee", "outputId": "f90ea1e3-2e19-478d-982a-a01ed6e3de62"}, "outputs": [], "source": ["# noise size\n", "NOISE_SIZE = 100\n", "IMAGE_SIZE = (28, 28)\n", "N_FILTERS = (256, 128, 64)\n", "\n", "# create a generator\n", "generator = make_generator_model(noise_size=NOISE_SIZE, image_size=IMAGE_SIZE, n_filters=N_FILTERS)\n", "\n", "# seed\n", "seed = tf.random.normal([1, NOISE_SIZE])\n", "\n", "# generate and plot the image\n", "generated_image = generator(seed, training=False)\n", "plt.figure(dpi=100)\n", "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n", "plt.axis('off')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "IPeLO5FX1Pee"}, "source": ["### The discriminator\n", "\n", "Next, we create the discriminator network. The discriminator takes an image and downsample it into a single real number between 0 and 1. The output being 0 means the discriminator classifies the image as fake, and 1 as real. We use `Conv2D` to downsample the images. \n", "\n", "**Takeaway**: the discriminator transforms an image into a real number."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "hzL8DZlj1Pef"}, "outputs": [], "source": ["def make_discriminator_model(image_size=(28,28), n_filters=(64,128)):\n", "    '''\n", "    Create a discriminator, hardcoded with two Conv2D layers.\n", "    \n", "    :param image_size: size of the image (both width and height must divide 4)\n", "    :param n_filters: number of filters in each layer\n", "    :return: the discriminator model\n", "    '''\n", "    \n", "    # sequential model\n", "    model = tf.keras.Sequential()\n", "    \n", "    # Conv2D\n", "    # input shape: (28, 28, 1)\n", "    # output shape: (14, 14, 64)\n", "    w = image_size[0]\n", "    h = image_size[1]\n", "    model.add(layers.Conv2D(n_filters[0], (5, 5), strides=(2, 2), padding='same',\n", "              input_shape=[w, h, 1]))\n", "    model.add(layers.LeakyReLU())\n", "    model.add(layers.Dropout(0.3))\n", "\n", "    # Conv2D\n", "    # input shape: (14, 14, 64)\n", "    # output shape: (7, 7, 128)\n", "    model.add(layers.Conv2D(n_filters[1], (5, 5), strides=(2, 2), padding='same'))\n", "    model.add(layers.LeakyReLU())\n", "    model.add(layers.Dropout(0.3))\n", "\n", "    # Flatten\n", "    # input shape: (7, 7, 128)\n", "    # output shape: (7*7*128,)\n", "    model.add(layers.Flatten())\n", "    \n", "    # 4th layer: Dense\n", "    # input shape: (7*7*128,)\n", "    # output shape: (1,)\n", "    model.add(layers.Dense(1))\n", "    \n", "    # input: (28, 28, 1)\n", "    # output: (1,)\n", "    return model"]}, {"cell_type": "markdown", "metadata": {"id": "W2I9hG3P1Pef"}, "source": ["Again, for size validation,  we can use the **untrained** discriminator to check the image we just generated. Note that we do not use `sigmoid` as the activation function of the last layer, so the output does not necessarily range between 0 and 1. Using `sigmoid` is also correct, but it slows down convergence at the early stage of training (as we want to save some time in this course)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "-7ixXWVh1Pef", "outputId": "692f59e5-cd67-426d-aa62-1e4fadd6146f"}, "outputs": [], "source": ["# create a discriminator\n", "discriminator = make_discriminator_model(image_size=(28,28), n_filters=(64,128))\n", "\n", "# check an image\n", "decision = discriminator(generated_image)\n", "print(decision)"]}, {"cell_type": "markdown", "metadata": {"id": "BJwU_Zuh1Peg"}, "source": ["### Loss functions\n", "\n", "This discriminator's loss quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s.\n", "\n", "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "qrr0OkpK1Peg"}, "outputs": [], "source": ["# cross entropy\n", "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n", "\n", "# discriminator's loss \n", "def discriminator_loss(real_output, fake_output):\n", "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n", "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n", "    total_loss = real_loss + fake_loss\n", "    return total_loss\n", "\n", "# generator's loss\n", "def generator_loss(fake_output):\n", "    return cross_entropy(tf.ones_like(fake_output), fake_output)"]}, {"cell_type": "markdown", "metadata": {"id": "LX6zYMxF1Peh"}, "source": ["### Optimizers\n", "\n", "The optimizer hyperparameters for the generator and the discriminator can be different. Here we use the same ones."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "YOpEfUyb1Peh"}, "outputs": [], "source": ["discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n", "generator_optimizer = tf.keras.optimizers.Adam(1e-4)"]}, {"cell_type": "markdown", "metadata": {"id": "j3ap7IM01Peh"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "RtnH9DnK1Peh"}, "source": ["# Training Loop\n", "\n", "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "NAkUe-lS1Pei"}, "outputs": [], "source": ["# Notice the use of `tf.function`\n", "# This annotation causes the function to be \"compiled\".\n", "@tf.function\n", "def train_step(real_images, noise_size):\n", "    # seed\n", "    batch_size = real_images.shape[0]\n", "    seed = tf.random.normal([batch_size, noise_size])\n", "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n", "        # fake images\n", "        fake_images = generator(seed, training=True)\n", "        \n", "        # discriminate real\n", "        real_score = discriminator(real_images, training=True)\n", "        # discriminate fake\n", "        fake_score = discriminator(fake_images, training=True)\n", "\n", "        # compute losses\n", "        gen_loss = generator_loss(fake_score)\n", "        disc_loss = discriminator_loss(real_score, fake_score)\n", "        \n", "        # compute gradients\n", "        gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n", "        disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n", "        \n", "        # apply gradients to update model parameters\n", "        generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n", "        discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n", "        \n", "        return gen_loss, disc_loss"]}, {"cell_type": "markdown", "metadata": {"id": "ofIjn6CI1Pei"}, "source": ["Next, we make a Tensorflow dataset from the raw image data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "AFT-meFW1Pei"}, "outputs": [], "source": ["BUFFER_SIZE = 60000\n", "BATCH_SIZE = 256\n", "\n", "train_dataset = tf.data.Dataset.from_tensor_slices(np.expand_dims(train_images.astype('float32'), -1))\n", "train_dataset = train_dataset.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True).batch(BATCH_SIZE)"]}, {"cell_type": "markdown", "metadata": {"id": "xtTLzXwn1Pei"}, "source": ["Now we can train the model. GANs converge slowly in general. Here we only do 50 epochs, after which the generated images start to resemble handwritten digits. Do more epochs to reach a better quality."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "q3OZcaDi1Pei", "outputId": "f3295c34-7627-49e4-a580-e107222674d6"}, "outputs": [], "source": ["############################\n", "######### TRAINING #########\n", "############################\n", "\n", "# epochs\n", "EPOCHS = 50\n", "\n", "# epoch loop\n", "tstart = time.time()\n", "for epoch in range(EPOCHS):\n", "    # batch loop\n", "    for i, image_batch in enumerate(train_dataset):\n", "        gen_loss, disc_loss = train_step(image_batch, NOISE_SIZE)\n", "    # print loss after each epoch\n", "    print(f'Epoch {epoch + 1} / {EPOCHS}, Elapsed = {time.time() - tstart:.2f} s, '\n", "          f'Gen loss = {gen_loss:.2f}, Disc loss = {disc_loss:.2f}')"]}, {"cell_type": "markdown", "metadata": {"id": "qXxx58MG1Pej"}, "source": ["Finally, we can use the trained generator to generate images from some random noise:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 693}, "id": "geHTVDtB1Pej", "outputId": "069fae32-6a92-41bd-e216-06a2aa321502"}, "outputs": [], "source": ["# seed\n", "nrows = 4\n", "ncols = 8\n", "seed = tf.random.normal([nrows * ncols, NOISE_SIZE])\n", "\n", "# generate images\n", "generated_image = generator(seed, training=False)\n", "\n", "# plot images\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot in range(nrows * ncols):\n", "    subplot_image(generated_image[iplot, :, :, 0], '', nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "v5L1J3mB1Pej"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "LsaoOtS01Pej"}, "source": ["## Exercises\n", "\n", "### Asynchronous Training\n", "Improve `train_step` into the following form to allow for asynchronous training:\n", "\n", "```python\n", "def train_step(real_images, noise_size, update_gen=True, update_disc=True):\n", "    ...\n", "```\n", "\n", "Here `update_gen` and `update_disc` are booleans to determine whether the generator and the discriminator should be updated in this step. Next, in the batch loop, use batch index `i` to determine the two booleans; for example, update the discriminator for every `i` and update the generator when `i%2=0`. \n", "\n", "\n", "### Wasserstein loss\n", "Implement the wasserstein loss:\n", "\n", "* Discriminator loss: $D(x)-D(G(z))$\n", "\n", "* Generator loss: $D(G(z))$\n", "\n", "\n", "### c-GAN\n", "We can extend our GAN into a conditional GAN or c-GAN so that we can specify the number to be generated. We must send the labels (0~9) to both the generator and discriminator.\n", "\n", "* For the generator, we can simply concatenate the label to the seed. Therefore, instead of doing\n", "\n", "    ```python\n", "    model.add(layers.Dense(w*h*n_filters[0], use_bias=False, \n", "                           input_shape=(noise_size,)))\n", "    ```\n", "\n", "    we can do \n", "\n", "    ```python\n", "    model.add(layers.Dense(w*h*n_filters[0], use_bias=False, \n", "                           input_shape=(noise_size + 1,)))\n", "    ```\n", "    \n", "    When calling the generator, concatenate the lables to the noise:\n", "    \n", "    ```python\n", "    fake_images = generator(tf.concat((seed, labels), axis=-1), training=True)\n", "    ```\n", "\n", "* For the discriminator, it is reported that one should add the labels as early as possible. The input is an image (either data or generated), so we can make the label into a (28, 28) image and append it to the input image as an additional channel, i.e., instead of doing\n", "\n", "    ```python\n", "    model.add(layers.Conv2D(n_filters[0], (5, 5), strides=(2, 2), padding='same',\n", "              input_shape=[w, h, 1]))\n", "    ```\n", "\n", "    we can do\n", "\n", "    ```python\n", "    model.add(layers.Conv2D(n_filters[0], (5, 5), strides=(2, 2), padding='same',\n", "              input_shape=[w, h, 2]))\n", "    ```\n", "    \n", "    When calling the discriminator, concatenate the lables to the images:\n", "    \n", "    ```python\n", "    labels_in_image_shape = tf.tile(tf.reshape(labels, [labels.shape[0], 1, 1, 1]), [1, 28, 28, 1])\n", "    real_score = discriminator(tf.concat((real_images, labels_in_image_shape), axis=-1), training=True)\n", "    fake_score = discriminator(tf.concat((fake_images, labels_in_image_shape), axis=-1), training=True)\n", "    ```\n", "    \n", "* The labels can be added to the TensorFlow dataset by\n", "\n", "    ```python\n", "    train_dataset = tf.data.Dataset.from_tensor_slices((np.expand_dims(train_images.astype('float32'), -1), \n", "                                                        np.expand_dims(train_labels.astype('float32'), -1)))\n", "    ```"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "KoSGHKCS1Pej"}, "outputs": [], "source": []}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "name": "GAN_basics.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 0}
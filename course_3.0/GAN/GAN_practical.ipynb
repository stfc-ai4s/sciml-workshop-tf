{"cells":[{"cell_type":"markdown","metadata":{"id":"IL9SW6kO1Pea"},"source":["# Generative Adversarial Networks: Generating Graphene EM Images\n","\n","In [VAE_practical.ipynb](../VAE/VAE_practical.ipynb), we have used the inelastic neutron scattering (INS) dataset to train a VAE to generate new images. In this notebook, we train a GAN with the same dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fhRLCYd81Peb","outputId":"d5a3cd60-b056-4d74-be4d-1efad8fb774b"},"outputs":[],"source":["# tensorflow\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","import tensorflow.keras.layers as layers\n","\n","# check version\n","print('Using TensorFlow v%s' % tf.__version__)\n","acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n","\n","# helpers\n","import h5py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","plt.style.use('ggplot')"]},{"cell_type":"markdown","metadata":{"id":"jJgy1Gy_yA6i"},"source":["## Google Cloud Storage Boilerplate\n","\n","The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n","\n","To access the data from Google Colab, you need to:\n","\n","1. Run the first cell;\n","2. Follow the link when prompted (you may be asked to log in with your Google account);\n","3. Copy the Google SDK token back into the prompt and press `Enter`;\n","4. Run the second cell and wait until the data folder appears.\n","\n","If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S90EzEdDyA6j"},"outputs":[],"source":["# variables passed to bash; do not change\n","project_id = 'sciml-workshop'\n","bucket_name = 'sciml-workshop-data'\n","colab_data_path = '/content/sciml-workshop-data/'\n","\n","try:\n","    from google.colab import auth\n","    auth.authenticate_user()\n","    google_colab_env = 'true'\n","    data_path = colab_data_path + 'sciml-workshop/'\n","except:\n","    google_colab_env = 'false'\n","    ###################################################\n","    ######## specify your local data path here ########\n","    ###################################################\n","    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ksGOHL1byA6k","outputId":"997afe4a-f8bc-456b-c1e8-54e1ea157920"},"outputs":[],"source":["%%bash -s {google_colab_env} {colab_data_path} {bucket_name} \n","\n","# running locally\n","if ! $1; then\n","    echo \"Running notebook locally.\"\n","    exit\n","fi\n","\n","# already mounted\n","if [ -d $2 ]; then\n","    echo \"Data already mounted.\"\n","    exit\n","fi\n","\n","apt -qq update\n","apt -qq install s3fs fuse\n","mkdir -p $2\n","s3fs $3 $2 -o allow_other,use_path_request_style,no_check_certificate,public_bucket=1,ssl_verify_hostname=0,host=https://s3.echo.stfc.ac.uk,url=https://s3.echo.stfc.ac.uk"]},{"cell_type":"markdown","metadata":{"id":"iagz8L7p1Pec"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"BecsOx5TyA6k"},"source":["# The dataset\n","\n","The INS dataset contains 20,000 images. This number is insufficient to train a GAN, given that the images are of high variability. Each image has a shape of `(20, 200)`, as composed of 10 images with shape `(20, 20)` in a row. To maximise the number of data, we divide one original image into 10 sub-images and filter out the weak ones. Finally, we obtain a dataset with 129,228 images."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"id":"_yuS7hPJ1Ped","outputId":"873a849b-f62a-4f97-8097-0ac1ffb39f79"},"outputs":[],"source":["# read original images\n","with h5py.File(data_path + 'ins-data/train.h5', 'r') as handle:\n","    big_images = handle['images'][:]\n","    \n","# divide into 10 sub-images\n","n = big_images.shape[0]\n","sub_images = np.moveaxis(big_images.reshape([n, 20, 10, 20]), 2, 1).reshape(n * 10, 20, 20, 1)\n","    \n","# plot an image and its sub-images\n","index = 0\n","plt.figure(dpi=200)\n","plt.imshow(big_images[index, :, :, 0])\n","plt.gca().set_aspect(1.)\n","plt.axis('off')\n","plt.show()\n","fig, ax = plt.subplots(1, 10, dpi=200)\n","for i in range(10):\n","    ax[i].imshow(sub_images[index * 10 + i, :, :, 0], vmin=0, vmax=1)\n","    ax[i].set_aspect(1.)\n","    ax[i].axis('off')\n","plt.show()\n","\n","# remove weak ones: max pixel smaller than a given threshold\n","threshold = 0.5\n","train_images = sub_images[np.where(np.max(sub_images, axis=(1, 2, 3)) > threshold)]\n","\n","# normalize to [-1, 1]\n","train_images = train_images * 2 - 1\n","\n","# print size info\n","print(f'Shape of original dataset: {big_images.shape}')\n","print(f'Shape of divided dataset: {sub_images.shape}')\n","print(f'Shape of training dataset: {train_images.shape}')"]},{"cell_type":"markdown","metadata":{"id":"D6wXheEGyA6m"},"source":["Next, we can create a dataset with the new image set:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jk0FI6dtyA6m"},"outputs":[],"source":["train_dataset = tf.data.Dataset.from_tensor_slices(train_images)\n","train_dataset = train_dataset.shuffle(1024)\n","train_dataset = train_dataset.batch(256)"]},{"cell_type":"markdown","metadata":{"id":"gXtKCXRf1Ped"},"source":["---\n","\n","# Create a GAN\n","\n","Our image size is `(20, 20)`, similar to `mnist`, so we may continue to use the network designed in [GAN_basics.ipynb](GAN_basics.ipynb). \n","\n","\n","### The generator\n","\n","Create a generator network. In the suggested answer, we simply copy from [GAN_basics.ipynb](GAN_basics.ipynb), only changing the image size.\n","\n","\n","**Suggested Answer** \n","\n","<details> <summary>Show / Hide</summary> \n","<p>\n","    \n","```python\n","def make_generator_model(noise_size=100, image_size=(28,28), n_filters=(256,128,64)):\n","    '''\n","    Create a generator, hardcoded with three Conv2DTranspose layers.\n","    \n","    :param noise_size: size of the seed vector\n","    :param image_size: size of the image (both width and height must divide 4)\n","    :param n_filters: number of filters in each layer\n","    :return: the generator model\n","    '''\n","    \n","    # sequential model\n","    model = tf.keras.Sequential()\n","    \n","    # Dense\n","    # input shape: (100,)\n","    # output shape: (7*7*256,)\n","    w = image_size[0] // 4\n","    h = image_size[1] // 4\n","    model.add(layers.Dense(w*h*n_filters[0], use_bias=False, \n","                           input_shape=(noise_size,)))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    # Reshape\n","    # input shape: (7*7*256,)\n","    # output shape: (7, 7, 256)\n","    model.add(layers.Reshape((w, h, n_filters[0])))\n","\n","    # Conv2DTranspose\n","    # input shape: (7, 7, 256)\n","    # output shape: (7, 7, 128)\n","    model.add(layers.Conv2DTranspose(n_filters[1], (5, 5), strides=(1, 1), \n","                                     padding='same', use_bias=False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    # Conv2DTranspose\n","    # input shape: (7, 7, 128)\n","    # output shape: (14, 14, 64)\n","    model.add(layers.Conv2DTranspose(n_filters[2], (5, 5), strides=(2, 2), \n","                                     padding='same', use_bias=False))\n","    model.add(layers.BatchNormalization())\n","    model.add(layers.LeakyReLU())\n","\n","    # Conv2DTranspose\n","    # input shape: (14, 14, 64)\n","    # output shape: (28, 28, 1)\n","    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), \n","                                     padding='same', use_bias=False, activation='tanh'))\n","    \n","    # input: (100,)\n","    # output: (28, 28, 1)\n","    return model\n","\n","# noise size\n","NOISE_SIZE = 100\n","\n","# create a generator\n","generator = make_generator_model(noise_size=NOISE_SIZE, image_size=(20, 20), \n","                                 n_filters=(256,128,64))\n","```\n","    \n","</p>\n","</details>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87jYoUd-1Ped"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"IPeLO5FX1Pee"},"source":["### The discriminator\n","\n","Create a discriminator network. In the suggested answer, we simply copy from [GAN_basics.ipynb](GAN_basics.ipynb), only changing the image size.\n","\n","**Suggested Answer** \n","\n","<details> <summary>Show / Hide</summary> \n","<p>\n","    \n","```python\n","def make_discriminator_model(image_size=(28,28), n_filters=(64,128)):\n","    '''\n","    Create a discriminator, hardcoded with two Conv2D layers.\n","    \n","    :param image_size: size of the image (both width and height must divide 4)\n","    :param n_filters: number of filters in each layer\n","    :return: the discriminator model\n","    '''\n","    \n","    # sequential model\n","    model = tf.keras.Sequential()\n","    \n","    # Conv2D\n","    # input shape: (28, 28, 1)\n","    # output shape: (14, 14, 64)\n","    w = image_size[0]\n","    h = image_size[1]\n","    model.add(layers.Conv2D(n_filters[0], (5, 5), strides=(2, 2), padding='same',\n","              input_shape=[w, h, 1]))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    # Conv2D\n","    # input shape: (14, 14, 64)\n","    # output shape: (7, 7, 128)\n","    model.add(layers.Conv2D(n_filters[1], (5, 5), strides=(2, 2), padding='same'))\n","    model.add(layers.LeakyReLU())\n","    model.add(layers.Dropout(0.3))\n","\n","    # Flatten\n","    # input shape: (7, 7, 128)\n","    # output shape: (7*7*128,)\n","    model.add(layers.Flatten())\n","    \n","    # 4th layer: Dense\n","    # input shape: (7*7*128,)\n","    # output shape: (1,)\n","    model.add(layers.Dense(1))\n","    \n","    # input: (28, 28, 1)\n","    # output: (1,)\n","    return model\n","\n","# create a discriminator\n","discriminator = make_discriminator_model(image_size=(20, 20), n_filters=(64,128))\n","```\n","    \n","</p>\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hzL8DZlj1Pef"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"BJwU_Zuh1Peg"},"source":["### Loss functions and optimizers\n","\n","Create the loss functions and optimizers for the generator and discriminator. In the suggested answer, we simply copy from [GAN_basics.ipynb](GAN_basics.ipynb) without changing anything.\n","\n","**Suggested Answer** \n","\n","<details> <summary>Show / Hide</summary> \n","<p>\n","    \n","```python\n","# cross entropy\n","cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","\n","# discriminator's loss \n","def discriminator_loss(real_output, fake_output):\n","    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n","    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n","    total_loss = real_loss + fake_loss\n","    return total_loss\n","\n","# generator's loss\n","def generator_loss(fake_output):\n","    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","\n","# optimizers\n","discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n","```\n","    \n","</p>\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qrr0OkpK1Peg"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"j3ap7IM01Peh"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"RtnH9DnK1Peh"},"source":["# Training Loop\n","\n","Implement the training function for a mini-batch. In the suggested answer, we simply copy from [GAN_basics.ipynb](GAN_basics.ipynb) without changing anything.\n","\n","**Suggested Answer** \n","\n","<details> <summary>Show / Hide</summary> \n","<p>\n","    \n","```python\n","# Notice the use of `tf.function`\n","# This annotation causes the function to be \"compiled\".\n","@tf.function\n","def train_step(real_images, noise_size):\n","    # seed\n","    batch_size = real_images.shape[0]\n","    seed = tf.random.normal([batch_size, noise_size])\n","    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","        # fake images\n","        fake_images = generator(seed, training=True)\n","        \n","        # discriminate real\n","        real_score = discriminator(real_images, training=True)\n","        # discriminate fake\n","        fake_score = discriminator(fake_images, training=True)\n","\n","        # compute losses\n","        gen_loss = generator_loss(fake_score)\n","        disc_loss = discriminator_loss(real_score, fake_score)\n","        \n","        # compute gradients\n","        gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n","        disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n","        \n","        # apply gradients to update model parameters\n","        generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n","        discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n","        \n","        return gen_loss, disc_loss\n","```\n","    \n","</p>\n","</details>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NAkUe-lS1Pei"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"xtTLzXwn1Pei"},"source":["Now perform the training. In the suggested answer, we simply copy from [GAN_basics.ipynb](GAN_basics.ipynb) without changing anything.\n","\n","**Suggested Answer** \n","\n","<details> <summary>Show / Hide</summary> \n","<p>\n","    \n","```python\n","############################\n","######### TRAINING #########\n","############################\n","\n","# epochs\n","EPOCHS = 50\n","\n","# epoch loop\n","tstart = time.time()\n","for epoch in range(EPOCHS):\n","    # batch loop\n","    for i, image_batch in enumerate(train_dataset):\n","        gen_loss, disc_loss = train_step(image_batch, NOISE_SIZE)\n","    # print loss after each epoch\n","    print(f'Epoch {epoch + 1} / {EPOCHS}, Elapsed = {time.time() - tstart:.2f} s, '\n","          f'Gen loss = {gen_loss:.2f}, Disc loss = {disc_loss:.2f}')\n","```\n","    \n","</p>\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3OZcaDi1Pei","outputId":"f07b53e9-1236-4909-e59a-8aa3aad22fe2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"qXxx58MG1Pej"},"source":["Finally, use the trained generator to generate images from random noise. In the suggested answer, we simply copy from [GAN_basics.ipynb](GAN_basics.ipynb). \n","\n","**Suggested Answer** \n","\n","<details> <summary>Show / Hide</summary> \n","<p>\n","    \n","```python\n","# seed\n","nrows = 10\n","ncols = 10\n","seed = tf.random.normal([nrows * ncols, NOISE_SIZE])\n","\n","# generate images\n","generated_image = generator(seed, training=False)\n","\n","# plot images\n","plt.figure(dpi=100, figsize=(ncols, nrows))\n","for iplot in range(nrows * ncols):\n","    plt.subplot(nrows, ncols, iplot + 1)\n","    plt.imshow(generated_image[iplot, :, :, 0])\n","    plt.xticks([])\n","    plt.yticks([])\n","plt.show()\n","```\n","    \n","</p>\n","</details>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":795},"id":"geHTVDtB1Pej","outputId":"d51ca2f7-afd6-4dea-ef8b-07c49a402f9b"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"dT4JSd1tyA6v"},"source":["Compare the generated results to those generated by a VAE in [VAE_practical.ipynb](../VAE/VAE_practical.ipynb). What can you find?"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"GAN_practical.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"nbformat":4,"nbformat_minor":1}

{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Classification using Decision Trees\n", "\n", "Decision trees are among the most popular classical machine learning algorithms, which can used for both classification (so-called classification trees) and regression (so-called regression trees). In this notebook, we will use decision trees for classification. \n", "\n", "Our implementation will be based on the open source machine learning library [scikit-learn](https://scikit-learn.org/stable/index.html). For simplicity and interpretability, we will use some synthetic 2D data in this notebook. In [regression_decision_tree.ipynb](regression_decision_tree.ipynb), we will use some scientific data to learn regression with decision trees."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# sklearn\n", "from sklearn.datasets import make_blobs\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.model_selection import GridSearchCV, train_test_split\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n", "import sklearn.datasets\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Helper Functions\n", "\n", "These are a few helper functions for generating data, plotting etc. You do not necessarily need to understand all the contents of these functions."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["def generate_toy_data():\n", "    centers= np.random.RandomState(0).uniform(-10, 10, size=(4, 2))\n", "    X, y = make_blobs(n_samples=300, n_features=2, centers=centers,\n", "                               cluster_std=1.5, random_state=0)\n", "    return X, y\n", "\n", "def plot_points(X, y, ax=None):\n", "    if ax is None:\n", "        fig, ax = plt.subplots()\n", "    scat = ax.scatter(X[:, 0], X[:, 1], c=y, s=20, alpha=0.7, edgecolors='k')\n", "    ax.set_xlabel('Feature 0')\n", "    ax.set_ylabel('Feature 1')\n", "    plt.gca().add_artist(plt.legend(*scat.legend_elements(), \n", "                                    title='Classes'))\n", "    \n", "# function to visualise classification\n", "def visualize_classifier(X, y, model, ax=None, cmap='viridis'):\n", "    ax = ax or plt.gca()\n", "    \n", "    # plot the training points\n", "    scat = ax.scatter(X[:, 0], X[:, 1], c=y, s=20, alpha=0.7, edgecolors='k',\n", "                      clim=(y.min(), y.max()), zorder=3)\n", "    ax.axis('tight')\n", "    ax.axis('off')\n", "    xlim = ax.get_xlim()\n", "    ylim = ax.get_ylim()\n", "    \n", "    # create a structured grid of the entire space\n", "    x0, x1 = np.meshgrid(np.linspace(*xlim, num=200), np.linspace(*ylim, num=200))\n", "    \n", "    # compute model predictions on the grid\n", "    y_pred = model.predict(np.c_[x0.ravel(), x1.ravel()]).reshape(x0.shape)\n", "\n", "    # create a contour plot of the grid\n", "    n_classes = len(np.unique(y))\n", "    ax.contourf(x0, x1, y_pred, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=cmap, zorder=1)\n", "    ax.set(xlim=xlim, ylim=ylim)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Data generation & Training/Test Split\n", "\n", "We start by generating a dataset using the `make_blobs` method from `sklearn`. This method can generate multi-dimensional random data points (blobs) in a Gaussian distribution around a specific number of centres.\n", "\n", "We split the data randomly by into a training and test set in the ratio 75/25.\n", "\n", "In the following cell, we generate a dataset of 300 samples in 2D (2 features) with 4 centres (four classes). "]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# generate Gaussian blobs\n", "X, y = generate_toy_data()\n", "\n", "# Split into training and test set\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n", "\n", "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n", "plot_points(X_train, y_train, ax=ax1)\n", "plot_points(X_train, y_train, ax=ax2)\n", "ax1.set_title('Training Set')\n", "ax2.set_title('Test Set')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Decision Tree\n", "\n", "## 1. Create a decision tree classifier\n", "\n", "Now we create a `DecisionTreeClassifier` object from `sklearn` to classify the blobs. There are many hyperparameters that can be set for the classifier. We will explain some important ones below. For a full list see [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n", "\n", "* **`max_depth`**\n", "\n", "The maximum depth of the tree. If left as `None`, nodes are expanded until all leaves are pure or until all leaves contain fewer than `min_samples_split` samples. Here we use `None`, the default value.\n", "\n", "* **`min_samples_split`**\n", "\n", "The minimum number of samples required to split an internal node. Here we use `min_samples_split=2`, the default value, so our tree will be a binary tree.\n", "    \n", "* **`max_features`**\n", "\n", "The maximum number of features considered for splitting. If left as `None`, the classifier will use `n_features` of the input data. Here we use `None`, the default value, so 4"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# create decision tree\n", "decision_tree = DecisionTreeClassifier(max_depth=None, min_samples_split=2, max_features=None)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Fit the model to data\n", "\n", "In this instance, we do not need to do much feature engineering because our data are relative simple with only two features and four classes. The `DecisionTreeClassifier` class has a `fit()` method, which takes the feature `X` and label `y` as inputs and fits the model to the data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# fit decision tree\n", "tree = decision_tree.fit(X, y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The following helper function can be used to visualise the result of the classification. It plots the training data points over a contour plot of model predictions over the entire space."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# visualise classification\n", "plt.figure(dpi=100)\n", "visualize_classifier(X, y, tree)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice that the classification looks rather arbitrary in the central region. To verify the robustness of the model, let us try re-running `fit()` with some random sub-sets of the training data:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Model evaluation\n", "    \n", "We can use the `accuracy_score` metric from `sklearn` to quantify the accuracy of the model on this data. Before computing `accuracy_score`, we first re-fit the model with the original `(X, y)` and make the predictions. This time, we are also returning the centres when generating the blobs, which are required later to generate the test dataset. \n", "\n", "### 3.1. Fitting with the training data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# re-fit model with (X_train, y_train)\n", "tree = decision_tree.fit(X_train, y_train)\n", "\n", "# make predictions\n", "y_pred = tree.predict(X_train)\n", "\n", "visualize_classifier(X_train, y_train, tree)\n", "\n", "# compute accuarcy score\n", "print('Accuracy score on training data: %.2f' % accuracy_score(y_train, y_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Pretty accurate! Wait up a moment -- let us try the model on some test data, generated with the same distribution parameters (including the centres) but a different `random_state` (giving different point locations):"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2. Fitting with the test data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# plot test data\n", "visualize_classifier(X_test, y_test, tree)\n", "\n", "# make predictions with test data\n", "y_test_pred = tree.predict(X_test)\n", "\n", "# compute accuarcy score with test data\n", "print('Accuracy score on test data: %.2f' % accuracy_score(y_test, y_test_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["That's not quite as good as before! The model is overfitting: it is performing well on the training set but not generalising beyond it."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Hyperparameter tuning and cross-validation\n", "\n", "Overfitting often results when the model has too many parameters and thus fits too exactly to the training set. To alleviate overfitting, we must choose a proper number of parameters in the model. For the `DecisionTreeClassifier` class, it is controlled by the `max_depth` argument. We are going to use a training/test split and cross-validation to tune this hyperparameter. The same method can be applied to the other hyperparameters. \n", "\n", "Below we set up a search across `max_depth` ranging from 1 to 20 by using the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) method. For each value, we use a 5-fold cross-validation."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# set up a parameter grid to search\n", "# here we only search max_depth\n", "param_grid = {'max_depth': range(1, 20)}\n", "\n", "# decision tree object\n", "decision_tree = DecisionTreeClassifier()\n", "\n", "# assoicte the gird with the object\n", "search = GridSearchCV(decision_tree, param_grid, cv=5)\n", "\n", "# perform search\n", "gs = search.fit(X, y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# plot the search result\n", "plt.figure(dpi=100)\n", "plt.plot(range(1, 20), search.cv_results_['mean_test_score'])\n", "plt.xlabel('Tree depth')\n", "plt.ylabel('Val Accuracy')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can see that the cross-validation score reaches its maximum at `max_depth=3`. We take the best performing hyperparameter and retrain the model on the full dataset. Then we can apply this tuned model to the test data and see if the accuracy improves."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can refit the model with the best parameters found by the grid search. We can use the dictionary `gs.best_params_` to access the values for the best parameters found during the hyper parameter search.   "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# get the best performing hyperparameter\n", "best_max_depth = gs.best_params_['max_depth']\n", "print('Best max_depth: %d' % best_max_depth)\n", "\n", "# create and fit model with the best performing hyperparameter\n", "tree = DecisionTreeClassifier(max_depth=best_max_depth).fit(X_train, y_train)\n", "\n", "# make predictions with test data\n", "y_test_pred = tree.predict(X_test)\n", "\n", "# compute accuarcy score with test data\n", "print('Accuracy score on test data: %.2f' % accuracy_score(y_test, y_test_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Advanced Concepts: Bagging and boosting\n", "\n", "We can improve the performance of decision trees in two ways, *bagging* and *boosting*, both belonging to ensemble learning algorithms. \"**Ensemble**\" means that the algorithm will plant not just one tree but an ensemble of trees. \n", "\n", "**Bagging** (or bootstrap aggregation) reduces the variance of a decision tree by training an ensemble of trees on random subsets of the training data (with some replacement) and taking the average of the ensemble as the final result. A **Random Forest** is an example of bagging: in addition to a random subset of data, a random forest uses a random subset of features to grow each tree in the ensemble. \n", "\n", "**Boosting**, as another ensemble technique, grows the trees in a sequential manner, that is, the misclassified samples or errors in the prior tree will be fed into the current tree and so forth; at the end, the final result is determined by majority voting from the ensemble. **AdaBoost** and **Gradient Boosting** are the most popular boosting algorithms. From a data perspective, the data points are weighted equally in bagging and unequally in boosting, as the former is parallel and the latter sequential. \n", "\n", "Here we will briefly demonstrate the usage of `RandomForestClassifier`  and `AdaBoostClassifier` from `sklearn`. Gradient boosting will be used for regression in [regression_decision_tree.ipynb](regression_decision_tree.ipynb). "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# create and fit a random forest classifier\n", "forest = RandomForestClassifier(n_estimators=300, random_state=0).fit(X, y)\n", "\n", "# visulise model\n", "plt.figure(dpi=100)\n", "visualize_classifier(X_test, y_test, forest)\n", "plt.show()\n", "\n", "# make predictions with test data\n", "y_test_pred = forest.predict(X_test)\n", "\n", "# compute accuarcy score with test data\n", "print('Accuracy score on test data: %.2f' % accuracy_score(y_test, y_test_pred))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# create and fit a AdaBoost classifier\n", "ada = AdaBoostClassifier(n_estimators=300, learning_rate=.003, random_state=0).fit(X, y)\n", "\n", "# visulise model\n", "plt.figure(dpi=100)\n", "visualize_classifier(X_train, y_train, ada)\n", "plt.show()\n", "\n", "# make predictions with test data\n", "y_test_pred = ada.predict(X_test)\n", "\n", "# compute accuarcy score with test data\n", "print('Accuracy score on test data: %.2f' % accuracy_score(y_test, y_test_pred))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises \n", "\n", "There are many standard \"toy\" datasets for practicing classification with machine learning, such as `iris`, `wine` and `breast_cancer`. Many of them can be directly loaded from `sklearn`. Try to solve one or some of these problems using decision trees (either single or ensemble). Among these datasets, `iris` is the simplest one with the fewest features and classes."]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ["# load iris dataset\n", "iris = sklearn.datasets.load_iris()\n", "print(iris['DESCR'])"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.10"}}, "nbformat": 4, "nbformat_minor": 4}
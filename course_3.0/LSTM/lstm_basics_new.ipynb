{"cells":[{"cell_type":"markdown","metadata":{"id":"0hjVNKY5cqPP"},"source":["# LSTM: The basics\n","\n","In this notebook, we will learn the basics of a Long Short Term Memory (LSTM) based on [Keras](https://keras.io/), a high-level API for building and training deep learning models, running on top of [TensorFlow](https://www.tensorflow.org/), an open source platform for machine learning. \n","\n","We will build a basic LSTM to predict stock prices in the future. The data is provided in your training environment - but in future you can also access it in [this github repo](https://github.com/mwitiderrick/stockprice).\n","\n","### Contents\n","\n","1. [Converting and preparing data](#convert_data)\n","2. [A multi-variate LSTM](#simple_lstm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYCCLvh0cqPU"},"outputs":[],"source":["# Importing the libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import Dropout\n","\n","plt.style.use('ggplot')"]},{"cell_type":"markdown","metadata":{"id":"o_KwpTqbcqPV"},"source":["## Google Cloud Storage Boilerplate\n","\n","The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n","\n","To access the data from Google Colab, you need to:\n","\n","1. Run the first cell;\n","2. Follow the link when prompted (you may be asked to log in with your Google account);\n","3. Copy the Google SDK token back into the prompt and press `Enter`;\n","4. Run the second cell and wait until the data folder appears.\n","\n","If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLlCYCHFcqPW"},"outputs":[],"source":["# variables passed to bash; do not change\n","project_id = 'sciml-workshop'\n","bucket_name = 'sciml-workshop-data'\n","colab_data_path = '/content/sciml-workshop-data/'\n","\n","try:\n","    from google.colab import auth\n","    auth.authenticate_user()\n","    google_colab_env = 'true'\n","    data_path = colab_data_path + 'sciml-workshop/'\n","except:\n","    google_colab_env = 'false'\n","    ###################################################\n","    ######## specify your local data path here ########\n","    ###################################################\n","    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USgu8B3WcqPX","outputId":"fc878711-c094-46fa-e151-0158c6d81de2"},"outputs":[],"source":["%%bash -s {google_colab_env} {colab_data_path} {bucket_name} \n","\n","# running locally\n","if ! $1; then\n","    echo \"Running notebook locally.\"\n","    exit\n","fi\n","\n","# already mounted\n","if [ -d $2 ]; then\n","    echo \"Data already mounted.\"\n","    exit\n","fi\n","\n","apt -qq update\n","apt -qq install s3fs fuse\n","mkdir -p $2\n","s3fs $3 $2 -o allow_other,use_path_request_style,no_check_certificate,public_bucket=1,ssl_verify_hostname=0,host=https://s3.echo.stfc.ac.uk,url=https://s3.echo.stfc.ac.uk"]},{"cell_type":"markdown","metadata":{"id":"SUKycDDmcqPY"},"source":["<a id='convert_data'></a>\n","## Converting data to sequence structure\n","\n","\n","One of the critical features of $RNNs$ and $LSTMs$ is that they work on sequences of data. In the lecture notes we saw that the network takes input at a time $t$ and the hiden layer state from $t-1$ and produces an output:\n","\n","<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/lstms-1.png?raw=1\" alt=\"lstms-1\" width=\"500\"/>\n","\n","However we may also want to include measured data from further back in time to help with remembering; so we could want input data from $t-1 \\cdots t-n$. We might also have more than one channel of input data, this is called the  **window** of the data. Imagine for example we were predicting stock prices, we could have the history of that stock, but we might also want to know the central bank interest rate, or the strength of one currency relative to another, in this case we have a **multi-variate** problem. So our input data looks more like:\n","\n","<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/lstms-2.png?raw=1\" alt=\"lstms-2\" width=\"500\"/>\n","\n","\n","Also, we might also want to make our $LSTM$ predict more than just one step forward, so we will want to be able to have multiple steps in the output."]},{"cell_type":"markdown","metadata":{},"source":["We write a function to convert dataframe series into data that is suitable for $LSTM$ training. This function is quite flexible and can be useful in many scenarios, so it is one that you might like to reuse in future if you are training time series models.\n","\n","As input we pass the data as a `numpy` array. We then also specify how far back in time we wish to look for each prediction `n_in`, so `n_in = 1` means we take just $t$ as input, `n_in=2` means we take $t, t-1$ as input and so on. We specify how far into the future we wish to predict, with the `n_out` variable. `n_out=1` means we predict for $t$, `n_out=2` means we predict for $t, t+1$ and so on. In addition to the window sizes, we can also select the features (columns in the data) to use with the `feature_indices` argument. \n","\n","The series data can be infinitely long while the memory of our LSTM cannot not be infinitely large. Therefore, we need to specify the length of the LSTM, or the `timesteps`, which is usually much smaller than the total length of the series. The first dimension of the outputs (`X` and `y` for `TensorFlow`) is `batch`; time-dependency is ignored among the batches (i.e., they can be shuffled)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaz0Sbv8cqPZ"},"outputs":[],"source":["def series_to_tensorflow(data, timesteps=10, n_in=1, n_out=1, feature_indices=None):\n","    \"\"\"\n","    Convert a series to tensorflow input.\n","    Arguments:\n","       data: Sequence of observations as a 2D NumPy array of shape (n_times, n_features)\n","       n_in: Window size of input X\n","       n_out: Window size of output y\n","       feature_indices: select features by indices; pass None to use all features\n","       timesteps: timesteps of LSTM\n","    Returns:\n","       X and y for tensorflow.keras.layers.LSTM\n","    \"\"\"\n","    # sizes\n","    n_total_times, n_total_features = data.shape[0], data.shape[1]\n","    n_batches = n_total_times - timesteps - (n_in - 1) - (n_out - 1)\n","    \n","    # feature selection\n","    if feature_indices is None:\n","        feature_indices = list(range(n_total_features))\n","    \n","    # data\n","    X = np.zeros((n_batches, timesteps, n_in, len(feature_indices)), dtype='float32')\n","    y = np.zeros((n_batches, n_out, len(feature_indices)), dtype='float32')\n","    for i_batch in range(n_batches):\n","        for i_in in range(n_in):\n","            X_start = i_batch + i_in\n","            X[i_batch, :, i_in, :] = data[X_start:X_start + timesteps, feature_indices]\n","        y_start = i_batch + timesteps + n_in - 1\n","        y[i_batch, :, :] = data[y_start:y_start + n_out, feature_indices]\n","    \n","    # flatten the last two dimensions\n","    return X.reshape(n_batches, timesteps, -1), y.reshape(n_batches, -1)"]},{"cell_type":"markdown","metadata":{"id":"0MIURIiecqPb"},"source":["<a id='simple_lstm'></a>\n","\n","## A multi-variate LSTM\n","\n","### Importing and treating the data\n","\n","We first read in our data and inspect it using `pandas`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2zhnRepbcqPb"},"outputs":[],"source":["# Importing the training set\n","dataset_train = pd.read_csv(data_path + '/lstm-data/data-train-lstm.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"C57mZoWxcqPc","outputId":"f9210dc5-f54e-4eab-b03a-eeee6403562d"},"outputs":[],"source":["dataset_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f'Dimensions of raw data: (n_times, n_features) = {dataset_train.shape}')"]},{"cell_type":"markdown","metadata":{"id":"Lu6MKgKKcqPd"},"source":["In the next cell, we get rid of the `Date` column (the first column) and normalize the rest price columns to `[0, 1]` using `MinMaxScaler` from `scikit-learn`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fitXgmTAcqPd"},"outputs":[],"source":["# drop `Date` column\n","values = dataset_train.drop(dataset_train.columns[[0,]], axis=1).values\n","\n","# normalize prices\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled = scaler.fit_transform(values)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f'Dimensions of normalized price data: (n_times, n_features) = {scaled.shape}')"]},{"cell_type":"markdown","metadata":{"id":"ugmmIj_BcqPd"},"source":["### Converting to $LSTM$ structure data\n","\n","\n","We now want to convert the data to a structure that can be fed to the $LSTM$. To do this we use our `series_to_tensorflow` function. In this case, we use 80 as the `timesteps`, looking back 8 steps (`n_in=8`) and projecting forward 4 steps (`n_out=4`) and considering the `Open` and `Close` prices (i.e., `feature_indices=[0, 4]`). "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6inbjtqpcqPe","outputId":"febcfa1a-43fe-4e23-c67a-7afd4bc7cd6b"},"outputs":[],"source":["timesteps = 80\n","n_in = 8\n","n_out = 4\n","feature_indices = [0, 4]\n","\n","X_train, y_train = series_to_tensorflow(scaled, timesteps=timesteps, \n","                                        n_in=n_in, n_out=n_out, feature_indices=feature_indices)\n","print(f'Dimensions of X_train: (batches, timesteps, features) = {X_train.shape}')\n","print(f'Dimensions of y_train: (batches, features) = {y_train.shape}')"]},{"cell_type":"markdown","metadata":{"id":"He68IW0qcqPf"},"source":["### Building the network\n","<a id='build_lstm'></a>\n","\n","Note that as before we start off from a `Sequential` type model in `Keras`.\n","\n","The $LSTM$ layers are already coded in `Keras` so we do not need to worry about writing the complicated structure. We just need to consider a few hyper-parametes we want to set,\n","\n","* Number of LSTM layers - we can stack LSTM layers in this case we will start with 2 stacked LSTMs;\n","* units - this is the dimensionality of the hidden state and memory cell of the LSTM;\n","* return_sequences - should we return the full output sequence or just the final value in the sequence. Generally, if the LSTM layer is feeding to another layer in the network, this would be `True`; if the LSTM layer is the final layer then this is `False`; note default is `False`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wfCnikimcqPf"},"outputs":[],"source":["# Initialising the LSTM\n","regressor = Sequential()\n","\n","# Adding the first LSTM layer\n","# shape: (N, 80, 16) => (N, 80, 50)\n","regressor.add(LSTM(units=50, return_sequences=True, dropout=0.4,\n","                   input_shape=(X_train.shape[1], X_train.shape[2])))\n","\n","# Adding a second LSTM layer\n","# shape: (N, 80, 50) => (N, 24)\n","regressor.add(LSTM(units=24, return_sequences=False, dropout=0.4,))\n","\n","# Adding an output layer \n","# shape: (N, 24) => (N, 8)\n","regressor.add(Dense(units=y_train.shape[1]))"]},{"cell_type":"markdown","metadata":{"id":"-pDxl6sjcqPg"},"source":["### Compile the network\n","\n","As ususal we need to compile the network, choosing an optimiser and a loss function. \n","\n","We use `adam` as our optimiser and `mean_squared_error` as our loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KbiDJj3tcqPg"},"outputs":[],"source":["# Compiling the RNN\n","regressor.compile(optimizer='adam', loss='mean_squared_error')"]},{"cell_type":"markdown","metadata":{"id":"_zdioBcfcqPg"},"source":["### Train the network"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0p6_8SYTcqPg","outputId":"2db8a6a7-fbbf-4da9-b29f-0561f1c16609"},"outputs":[],"source":["# Fitting the RNN to the Training set\n","history = regressor.fit(X_train, y_train, epochs=30, batch_size=128)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"id":"mE-ngYaOcqPh","outputId":"48908b70-2d3c-44d0-d3a7-8d69ae3ad86f"},"outputs":[],"source":["plt.figure(dpi=100)\n","plt.plot(history.history['loss'])\n","plt.xlabel('epoch')\n","plt.ylabel('mae')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-Qegdaf2cqPh"},"source":["### Making predictions with our model\n","\n","We now use model that we just built to predict on previously un-seen data. We read in `data-test.csv` and get the stock prices from that data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P4a_AFTMcqPh"},"outputs":[],"source":["# Part 3 - Making the predictions and visualising the results\n","\n","# Getting the real stock price of 2017\n","dataset_test = pd.read_csv(data_path + 'lstm-data/data-test-lstm.csv')\n","real_stock_price = dataset_test.iloc[:, 1:2].values\n","print(dataset_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_test.head()"]},{"cell_type":"markdown","metadata":{},"source":["The testing data contain 16 time steps. We will concatenate them to the training data for time extension. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Getting the predicted stock price of 2017\n","dataset_extended = pd.concat((dataset_train, dataset_test), axis=0)\n","values_extended = dataset_extended.drop(dataset_extended.columns[[0,]], axis=1).values\n","scaled_extended = scaler.transform(values_extended)\n","print(f'Dimensions of normalized price data: (n_times, n_features) = {scaled_extended.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# using the last points to generate testing data\n","X_test, y_test = series_to_tensorflow(scaled_extended[-(timesteps + n_in + n_out - 2 + len(dataset_test)):], \n","                                      timesteps=timesteps,\n","                                      n_in=n_in, n_out=n_out, feature_indices=feature_indices)\n","\n","# make predictions\n","y_pred = regressor(X_test)\n","print(f'Dimensions of X_test: (batches, timesteps, features) = {X_test.shape}')\n","print(f'Dimensions of y_test: (batches, features) = {y_test.shape}')\n","print(f'Dimensions of y_pred: (batches, features) = {y_pred.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":462},"id":"AcrVDx2-cqPi","outputId":"d3073c8f-2531-481b-8fb5-f89df94f3412"},"outputs":[],"source":["# Visualising the results\n","fig, ax = plt.subplots(1, len(feature_indices), dpi=100, \n","                       figsize=(5 * len(feature_indices), 3), squeeze=False)\n","for i, index in enumerate(feature_indices):\n","    # we are ploting the last point from the output window\n","    ax[0, i].plot(y_test[:, i + (n_out - 1) * len(feature_indices)], \n","                  label = 'Real ' + dataset_train.keys()[index + 1])\n","    ax[0, i].plot(y_pred[:, i + (n_out - 1) * len(feature_indices)], \n","                  label = 'Pred ' + dataset_train.keys()[index + 1])\n","    ax[0, i].legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"F_EpvUpqcqPi"},"source":["### Exercise\n","\n","* Build a network with three or four $LSTM$ layers. How does this affect the performance?\n","* Change the LSTM timesteps (`timesteps`) - how does a longer/shorter one affect the predictions?\n","* Try other window sizes and features.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eo6Puc5tcqPm"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"lstm_basics.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}

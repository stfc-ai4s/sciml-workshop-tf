{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# CNN: the Basics\n", "\n", "A convolutional neural network (CNN) is a deep-learning algorithm frequently used for analysing image data. In this notebook, we will learn the basics of a CNN and apply it to classify the `fashion-mnist` dataset.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import skimage\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "plt.style.use('ggplot')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Convolutional Filter\n", "\n", "A convolutional filter is the basic element (or neuron) of a CNN. To better understand CNN, we first learn how a convolutional filter works by hand coding it.\n", "\n", "### The kernel\n", "\n", "A convolutional filter extracts a part of the input image and inner-product it with a **kernel** to fill one pixel in the output image. The process is illustrated in the following figure. The behaviour of a convolutional filter is predominated by its kernel. *For image processing, we need to specify the kernel as an input parameter. In a CNN, however, we only specify the size of the kernels whereas their values are learnt by training.*\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/conv3x3.png?raw=1\" width=\"80%\">\n", "\n", "### Padding and stride\n", "\n", "In addition to the kernel, there are some other useful parameters, such as:\n", "\n", "* **Padding**: padding zeros around the input image to preserve (or even increase) the image size, e.g., when padding = 1:\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/padding.png?raw=1\" width=\"90%\">\n", "\n", "* **Stride**: it controls how fast the kernel moves over the input image and thus the size of the output image, e.g., when stride = 2:\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/strides.png?raw=1\" width=\"50%\">"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Implement a convolutional filter\n", "#### Input\n", "* `input_image`: an input image with shape (nx, ny, nchannel)\n", "* `kernel`: a square matrix with shape (k, k)\n", "* `padding`: a non-negative integer\n", "* `stride`: a positive integer; to sample the right edge of the input image, it must divide (nx + padding * 2 - k), similarly for the bottom edge; it also controls the output resolution and the computational cost\n", "\n", "#### Output\n", "* `return`: an output image with shape (nx_out, ny_out, nchannel), where nx_out = (nx + padding * 2 - k) // stride + 1 and ny_out = (ny + padding * 2 - k) // stride + 1\n", "\n", "**NOTE**: For readability, the code is a dry implementation without much optimisation, so its performance is not high. Increase `stride` to speedup the processing at the cost of a downsampled output image."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# a 2D convolutonal filter\n", "def convolve2D(input_image, kernel, padding=1, stride=1):\n", "    # padding\n", "    nx = input_image.shape[0]\n", "    ny = input_image.shape[1]\n", "    nchannel = input_image.shape[2]\n", "    if padding > 0:\n", "        padded_image = np.zeros((nx + padding * 2, ny + padding * 2, nchannel))\n", "        padded_image[padding:-padding, padding:-padding, :] = input_image\n", "    else:\n", "        padded_image = input_image\n", "    \n", "    # allocate output\n", "    k = kernel.shape[0]\n", "    nx_out = (nx + padding * 2 - k) // stride + 1 # must use // instead of /\n", "    ny_out = (ny + padding * 2 - k) // stride + 1\n", "    output_image = np.zeros((nx_out, ny_out, nchannel))\n", "    \n", "    # compute output pixel by pixel\n", "    for ix_out in np.arange(nx_out):\n", "        for iy_out in np.arange(ny_out):\n", "            ix_in = ix_out * stride\n", "            iy_in = iy_out * stride\n", "            # the inner product\n", "            output_image[ix_out, iy_out, :] = \\\n", "            np.tensordot(kernel, padded_image[ix_in:(ix_in + k), iy_in:(iy_in + k), :], axes=2)\n", "    \n", "    # truncate to [0, 1]\n", "    output_image = np.maximum(output_image, 0)\n", "    output_image = np.minimum(output_image, 1)\n", "    return output_image"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Apply our convolutional filter\n", "\n", "Next, we load an image from `skimage.data` and apply our convolutional filter to it. Here we will use the 3$\\times$3 Sobel kernel, which is good at edge detection:\n", "\n", ">$k=\\begin{bmatrix}\n", "  1 & 0 & -1\\\\ \n", "  2 & 0 & -2\\\\\n", "  1 & 0 & -1\n", "\\end{bmatrix}$ "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# load some image\n", "input_image = skimage.data.coffee()\n", "input_image = input_image / 255.\n", "\n", "# print image size\n", "print('Image pixels: %d x %d' % (input_image.shape[0], input_image.shape[1]))\n", "print('Channels (RGB): %d' % (input_image.shape[2]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# vertical Sobel kernel\n", "kernel = np.array([\n", "    [1, 0, -1],\n", "    [2, 0, -2],\n", "    [1, 0, -1]])\n", "\n", "##################################\n", "# Also try the following kernels #\n", "##################################\n", "\n", "# # horizontal Sobel kernel\n", "# kernel = np.array([\n", "#     [1, 2, 1],\n", "#     [0, 0, 0],\n", "#     [-1, -2, -1]])\n", "\n", "# # smoothening\n", "# kernel = np.array([\n", "#     [1, 1, 1],\n", "#     [1, 1, 1],\n", "#     [1, 1, 1]]) / 9\n", "\n", "# # sharpening\n", "# kernel = np.array([\n", "#     [0, -1, 0],\n", "#     [-1, 5, -1],\n", "#     [0, -1, 0]])\n", "\n", "\n", "#######################\n", "# Try a larger stride #\n", "#######################\n", "# do convolution\n", "output_image = convolve2D(input_image, kernel, padding=1, stride=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# plot original image\n", "plt.figure(dpi=100, figsize=(12, 4))\n", "plt.subplot(1, 2, 1)\n", "plt.imshow(input_image)\n", "plt.axis('off')\n", "plt.title('Original (%d x %d)' % (input_image.shape[0], input_image.shape[1]))\n", "\n", "# plot convolved image\n", "plt.subplot(1, 2, 2)\n", "plt.imshow(output_image)\n", "plt.axis('off')\n", "plt.title('Convolved (%d x %d)' % (output_image.shape[0], output_image.shape[1]))\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The above results show that the Sobel kernel can depict the outline of the objects. The capability of detecting object features by associating neighboring pixels makes CNNs powerful in analysing image data.\n", "\n", "\n", "### Exercise\n", "\n", "The vertical and the horizontal Sobel kernels can be superposed to make an inclined-edge detecting kernel:\n", "\n", ">$k(\\theta)=\\cos(\\theta)\\begin{bmatrix}\n", "  1 & 2 & 1\\\\ \n", "  0 & 0 & 0\\\\\n", "  -1 & -2 & -1\n", "\\end{bmatrix}\n", "+\n", "\\sin(\\theta)\\begin{bmatrix}\n", "  1 & 0 & -1\\\\ \n", "  2 & 0 & -2\\\\\n", "  1 & 0 & -1\n", "\\end{bmatrix},\n", "$\n", "> where $\\theta$ is the angle from horizontal.\n", "\n", "Find a kernel to erase most of the stripes on the table.\n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# CNN for `fashion-mnist`\n", "\n", "Now we train a CNN to classify the `fashion-mnist` dataset, following the same steps elaborated in [DNN_basics.ipynb](../DNN/DNN_basics.ipynb). "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0. Load the dataset\n", "\n", "As before, we start by loading the dataset. Note that we are appending a channel dimension to the images as required by the convolutional layers."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# load dataset\n", "fashion_mnist = keras.datasets.fashion_mnist\n", "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n", "\n", "# normalise images\n", "train_images = train_images / 255.0\n", "test_images = test_images / 255.0\n", "\n", "# append a channel dimension as required by Conv2D\n", "train_images = np.array([train_images]).transpose([1, 2, 3, 0])\n", "test_images = np.array([test_images]).transpose([1, 2, 3, 0])\n", "\n", "# string labels\n", "string_labels = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n", "                 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n", "\n", "# print info\n", "print(\"Number of training data: %d\" % len(train_labels))\n", "print(\"Number of test data: %d\" % len(test_labels))\n", "print(\"Image pixels: %s\" % str(train_images[0, :, :, 0].shape))\n", "print(\"Number of channels: %s\" % str(train_images.shape[-1]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# function to plot an image in a subplot\n", "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n", "    plt.subplot(nrows, ncols, iplot + 1)\n", "    plt.imshow(image.squeeze(), cmap=plt.cm.binary)\n", "    plt.xlabel(label, c='k', fontsize=12)\n", "    plt.title(label2, c=label2_color, fontsize=12, y=-0.33)\n", "    plt.xticks([])\n", "    plt.yticks([])\n", "    \n", "# ramdomly plot some images and their labels\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n", "    label = \"%d: %s\" % (train_labels[idata], string_labels[train_labels[idata]])\n", "    subplot_image(train_images[idata], label, nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Build the network architecture\n", "\n", "Our CNN contains two convolutional layers, each followed by a max-pooling layer and a batch-normalisation layer, and then a dense layer with dropout and finally the output layer. The network architecture is shown in the following figure ([figure source](https://mc.ai/the-convolution-parameters-calculation/)):\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/layer.jpeg?raw=1\">\n", "\n", "\n", "We have understood how a convolutional filter works. A convolutional layer is simply a collection of convolutional filters whose kernel values form the trainable parameters. The most frequently used kernel sizes are 3$\\times$3, 5$\\times$5 and 7$\\times$7. The number of filters in each layer is a key network parameter governing the model size. \n", "\n", "The *max-pooling* layers are aimed for dimensionality reduction, containing no trainable parameters. It can be easily understood with the following illustration ([figure source](https://medium.com/ai-in-plain-english/pooling-layer-beginner-to-intermediate-fa0dbdce80eb)). The most common size for max pooling is 2$\\times$2.\n", "\n", "<img src=\"https://github.com/stfc-sciml/sciml-workshop/blob/master/course_3.0_with_solutions/markdown_pic/maxpooling.png?raw=1\" width=\"50%\">\n", "\n", "The *batch-normalisation* layers can help the CNN to converge faster and become more stable through normalisation of the input layer by re-centering and re-scaling."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the network architecture\n", "model = Sequential()\n", "model.add(Conv2D(16, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n", "model.add(MaxPool2D((2, 2)))\n", "model.add(BatchNormalization())\n", "model.add(Conv2D(32, (5, 5), activation='relu'))\n", "model.add(MaxPool2D((2, 2)))\n", "model.add(BatchNormalization())\n", "model.add(Flatten())\n", "model.add(Dense(128, activation='relu'))\n", "model.add(Dropout(0.5))\n", "model.add(Dense(10, activation='sigmoid'))\n", "\n", "# print summary\n", "model.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Compile the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# optimizer, loss, metrics\n", "model.compile(optimizer='adam',\n", "              loss=keras.losses.SparseCategoricalCrossentropy(),\n", "              metrics=['accuracy'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Train the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# train the model\n", "training_history = model.fit(train_images, train_labels, epochs=10, batch_size=32, \n", "                             validation_data=(test_images, test_labels))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# plot accuracy\n", "plt.figure(dpi=100, figsize=(12, 4))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(training_history.history[acc_str], label='Accuracy on training data')\n", "plt.plot(training_history.history['val_' + acc_str], label='Accuracy on test data')\n", "plt.legend()\n", "plt.title(\"Accuracy\")\n", "\n", "# plot loss\n", "plt.subplot(1, 2, 2)\n", "plt.plot(training_history.history['loss'], label='Loss on training data')\n", "plt.plot(training_history.history['val_loss'], label='Loss on test data')\n", "plt.legend()\n", "plt.title(\"Loss\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compared to our fully connected DNN in [DNN_basics.ipynb](../DNN/DNN_basics.ipynb), our CNN turns out more efficient. It can achieve a higher accuracy with much fewer epochs."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Make predictions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# use test images to predict\n", "pred_lables = model.predict(test_images).argmax(axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# get the indices of wrong predictions\n", "id_wrong = np.where(pred_lables != test_labels)[0]\n", "print(\"Number of test data: %d\" % test_labels.size)\n", "print(\"Number of wrong predictions: %d\" % id_wrong.size)\n", "\n", "# plot the wrong predictions\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(id_wrong, nrows * ncols)):\n", "    label = \"%d: %s\" % (test_labels[idata], string_labels[test_labels[idata]])\n", "    label2 = \"%d: %s\" % (pred_lables[idata], string_labels[pred_lables[idata]])\n", "    subplot_image(test_images[idata], label, nrows, ncols, iplot, label2, 'r')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Exercises\n", "\n", "Classify a more complicated dataset, e.g., a randomly rotated `fashion-mnist` as shown below. For the augmented dataset, compare the accuracy of a CNN and a fully connected DNN.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# rotate images randomly by k * 90 degrees\n", "# train\n", "train_images_rot = train_images.copy()\n", "for i in np.arange(train_images.shape[0]):\n", "    train_images_rot[i] = np.rot90(train_images[i], k=np.random.choice([0, 1, 2, 3]))\n", "    \n", "# test\n", "test_images_rot = test_images.copy()\n", "for i in np.arange(test_images.shape[0]):\n", "    test_images_rot[i] = np.rot90(test_images[i], k=np.random.choice([0, 1, 2, 3]))\n", "    \n", "# ramdomly plot some images and their labels\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n", "    label = \"%d: %s\" % (train_labels[idata], string_labels[train_labels[idata]])\n", "    subplot_image(train_images_rot[idata], label, nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}